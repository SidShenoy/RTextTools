db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument)
# I am using SVM after cleaning the data (i.e. removal of stop words, punctuations, stemming, etc.) and on unigrams only!
# There is a problem with "RTextTools" package for ngrams, so I am using unigrams only.
# There are 7 steps associated with "RTextTools package" for classification ->
# 1.  Obtaining the data for both training and testing.
# 2.  Creating the DTM for training data.
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
# 4.  Creating the model using the container of step 3.
# 5.  Creating the DTM for testing data based on training data.
# 6.  Creating the container for the testing data(specifying their labels as NULL/zero and how many of them are to be predicted/tested?)
# 7.  Providing the "testing" container of step 6 as input to the model created in step 4 to get output.
# 2.  Creating the DTM for training data.
# Mentioning the starting document index for training
start_doc_index_for_training = 1
# Mentioning the ending document index for training
end_doc_index_for_training = 100
trainSize = end_doc_index_for_training - start_doc_index_for_training + 1
train_db_corpus_for_SVM = c(corpus.ng.train.db[[1]]["content"][[1]])
if(length(db_train_docs)>1)
{
for(i in 2:length(db_train_docs))
{
train_db_corpus_for_SVM = c(train_db_corpus_for_SVM, paste(corpus.ng.train.db[[i]]["content"][[1]], collapse = ''))
}
}
dtMatrix.db <- create_matrix(train_db_corpus_for_SVM[start_doc_index_for_training:end_doc_index_for_training], removeSparseTerms = 0.99)
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
db_train_corpus_data_labels = rep(1,length(db_train_docs))
db_train_corpus_data_labels[c(1,11,17,36,52,69,79,80)] = 2
container.db <- create_container(dtMatrix.db, db_train_corpus_data_labels, trainSize=start_doc_index_for_training:end_doc_index_for_training, virgin=FALSE)
# 4.  Creating the model (SVM) using the container of step 3.
model.db <- train_model(container.db, "SVM", kernel="linear", cost=1)
types_of_labels = c(1, 2)
label_names = c("Accepted", "Rejected")
#Obtaining the full path to the "Case Data Files" local directory
full_path_to_data_files = paste("/media/sidshenoy/KINGSTON/NIC work/SVM Classifier App", "/Case Data Files", sep = "")
#obtaining the names of all files ending with "pdf"
db_files <- list.files(path = full_path_to_data_files, pattern = "pdf$")
#obtaining the full paths to all the above files
db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument)
# I am using SVM after cleaning the data (i.e. removal of stop words, punctuations, stemming, etc.) and on unigrams only!
# There is a problem with "RTextTools" package for ngrams, so I am using unigrams only.
# There are 7 steps associated with "RTextTools package" for classification ->
# 1.  Obtaining the data for both training and testing.
# 2.  Creating the DTM for training data.
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
# 4.  Creating the model using the container of step 3.
# 5.  Creating the DTM for testing data based on training data.
# 6.  Creating the container for the testing data(specifying their labels as NULL/zero and how many of them are to be predicted/tested?)
# 7.  Providing the "testing" container of step 6 as input to the model created in step 4 to get output.
# 2.  Creating the DTM for training data.
# Mentioning the starting document index for training
start_doc_index_for_training = 1
# Mentioning the ending document index for training
end_doc_index_for_training = 100
trainSize = end_doc_index_for_training - start_doc_index_for_training + 1
train_db_corpus_for_SVM = c(corpus.ng.train.db[[1]]["content"][[1]])
if(length(db_train_docs)>1)
{
for(i in 2:length(db_train_docs))
{
train_db_corpus_for_SVM = c(train_db_corpus_for_SVM, paste(corpus.ng.train.db[[i]]["content"][[1]], collapse = ''))
}
}
dtMatrix.db <- create_matrix(train_db_corpus_for_SVM[start_doc_index_for_training:end_doc_index_for_training])
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
db_train_corpus_data_labels = rep(1,length(db_train_docs))
db_train_corpus_data_labels[c(1,11,17,36,52,69,79,80)] = 2
container.db <- create_container(dtMatrix.db, db_train_corpus_data_labels, trainSize=start_doc_index_for_training:end_doc_index_for_training, virgin=FALSE)
# 4.  Creating the model (SVM) using the container of step 3.
model.db <- train_model(container.db, "SVM", kernel="linear", cost=1)
types_of_labels = c(1, 2)
label_names = c("Accepted", "Rejected")
#Obtaining the full path to the "Case Data Files" local directory
full_path_to_data_files = paste("/media/sidshenoy/KINGSTON/NIC work/SVM Classifier App", "/Case Data Files", sep = "")
#obtaining the names of all files ending with "pdf"
db_files <- list.files(path = full_path_to_data_files, pattern = "pdf$")
#obtaining the full paths to all the above files
db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument, lazy = TRUE)
#Obtaining the full path to the "Case Data Files" local directory
full_path_to_data_files = paste("/media/sidshenoy/KINGSTON/NIC work/SVM Classifier App", "/Case Data Files", sep = "")
#obtaining the names of all files ending with "pdf"
db_files <- list.files(path = full_path_to_data_files, pattern = "pdf$")
#obtaining the full paths to all the above files
db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument, lazy = TRUE)
# I am using SVM after cleaning the data (i.e. removal of stop words, punctuations, stemming, etc.) and on unigrams only!
# There is a problem with "RTextTools" package for ngrams, so I am using unigrams only.
# There are 7 steps associated with "RTextTools package" for classification ->
# 1.  Obtaining the data for both training and testing.
# 2.  Creating the DTM for training data.
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
# 4.  Creating the model using the container of step 3.
# 5.  Creating the DTM for testing data based on training data.
# 6.  Creating the container for the testing data(specifying their labels as NULL/zero and how many of them are to be predicted/tested?)
# 7.  Providing the "testing" container of step 6 as input to the model created in step 4 to get output.
# 2.  Creating the DTM for training data.
# Mentioning the starting document index for training
start_doc_index_for_training = 1
# Mentioning the ending document index for training
end_doc_index_for_training = 100
trainSize = end_doc_index_for_training - start_doc_index_for_training + 1
train_db_corpus_for_SVM = c(corpus.ng.train.db[[1]]["content"][[1]])
if(length(db_train_docs)>1)
{
for(i in 2:length(db_train_docs))
{
train_db_corpus_for_SVM = c(train_db_corpus_for_SVM, paste(corpus.ng.train.db[[i]]["content"][[1]], collapse = ''))
}
}
dtMatrix.db <- create_matrix(train_db_corpus_for_SVM[start_doc_index_for_training:end_doc_index_for_training])
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
db_train_corpus_data_labels = rep(1,length(db_train_docs))
db_train_corpus_data_labels[c(1,11,17,36,52,69,79,80)] = 2
container.db <- create_container(dtMatrix.db, db_train_corpus_data_labels, trainSize=start_doc_index_for_training:end_doc_index_for_training, virgin=FALSE)
# 4.  Creating the model (SVM) using the container of step 3.
model.db <- train_model(container.db, "SVM", kernel="linear", cost=1)
types_of_labels = c(1, 2)
label_names = c("Accepted", "Rejected")
install.packages("SnowballC")
#Obtaining the full path to the "Case Data Files" local directory
full_path_to_data_files = paste("/media/sidshenoy/KINGSTON/NIC work/SVM Classifier App", "/Case Data Files", sep = "")
#obtaining the names of all files ending with "pdf"
db_files <- list.files(path = full_path_to_data_files, pattern = "pdf$")
#obtaining the full paths to all the above files
db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument, lazy = TRUE)
# I am using SVM after cleaning the data (i.e. removal of stop words, punctuations, stemming, etc.) and on unigrams only!
# There is a problem with "RTextTools" package for ngrams, so I am using unigrams only.
# There are 7 steps associated with "RTextTools package" for classification ->
# 1.  Obtaining the data for both training and testing.
# 2.  Creating the DTM for training data.
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
# 4.  Creating the model using the container of step 3.
# 5.  Creating the DTM for testing data based on training data.
# 6.  Creating the container for the testing data(specifying their labels as NULL/zero and how many of them are to be predicted/tested?)
# 7.  Providing the "testing" container of step 6 as input to the model created in step 4 to get output.
# 2.  Creating the DTM for training data.
# Mentioning the starting document index for training
start_doc_index_for_training = 1
# Mentioning the ending document index for training
end_doc_index_for_training = 100
trainSize = end_doc_index_for_training - start_doc_index_for_training + 1
train_db_corpus_for_SVM = c(corpus.ng.train.db[[1]]["content"][[1]])
if(length(db_train_docs)>1)
{
for(i in 2:length(db_train_docs))
{
train_db_corpus_for_SVM = c(train_db_corpus_for_SVM, paste(corpus.ng.train.db[[i]]["content"][[1]], collapse = ''))
}
}
dtMatrix.db <- create_matrix(train_db_corpus_for_SVM[start_doc_index_for_training:end_doc_index_for_training])
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
db_train_corpus_data_labels = rep(1,length(db_train_docs))
db_train_corpus_data_labels[c(1,11,17,36,52,69,79,80)] = 2
container.db <- create_container(dtMatrix.db, db_train_corpus_data_labels, trainSize=start_doc_index_for_training:end_doc_index_for_training, virgin=FALSE)
# 4.  Creating the model (SVM) using the container of step 3.
model.db <- train_model(container.db, "SVM", kernel="linear", cost=1)
types_of_labels = c(1, 2)
label_names = c("Accepted", "Rejected")
totalSize <- sort(unique(append(start_doc_index_for_training:end_doc_index_for_training,NULL)))
column_names <- colnames(matrix)
data_matrix <- as.compressed.matrix(matrix[totalSize])
totalSize
column_names <- colnames(dtMatrix.db)
column_names
data_matrix <- as.compressed.matrix(dtMatrix.db[totalSize])
dtMatrix.db[totalSize]
totalSize
totalSize[3]
dtMatrix.db[1:3]
dtMatrix.db[1]
View(dtMatrix.db)
dtMatrix.db[[1]]
dtMatrix.db[[2]]
dtMatrix.db[[3]]
?create_container
install.packages("/media/sidshenoy/KINGSTON/NIC work/Latest work/RTextTools_1.4.2.tar.gz", repos = NULL, type = "source")
detach("package:RTextTools", unload=TRUE)
library("RTextTools", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
detach("package:RTextTools", unload=TRUE)
install.packages("/media/sidshenoy/KINGSTON/NIC work/Latest work/RTextTools_1.4.2.tar.gz", repos = NULL, type = "source")
remove.packages("RTextTools", lib="~/R/x86_64-pc-linux-gnu-library/3.5")
install.packages("/media/sidshenoy/KINGSTON/NIC work/Latest work/RTextTools_1.4.2.tar.gz", repos = NULL, type = "source")
library("RTextTools", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
remove.packages("RTextTools", lib="~/R/x86_64-pc-linux-gnu-library/3.5")
library("RTextTools", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
totalSize
#Obtaining the full path to the "Case Data Files" local directory
full_path_to_data_files = paste("/media/sidshenoy/KINGSTON/NIC work/SVM Classifier App", "/Case Data Files", sep = "")
#obtaining the names of all files ending with "pdf"
db_files <- list.files(path = full_path_to_data_files, pattern = "pdf$")
#obtaining the full paths to all the above files
db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument, lazy = TRUE)
# I am using SVM after cleaning the data (i.e. removal of stop words, punctuations, stemming, etc.) and on unigrams only!
# There is a problem with "RTextTools" package for ngrams, so I am using unigrams only.
# There are 7 steps associated with "RTextTools package" for classification ->
# 1.  Obtaining the data for both training and testing.
# 2.  Creating the DTM for training data.
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
# 4.  Creating the model using the container of step 3.
# 5.  Creating the DTM for testing data based on training data.
# 6.  Creating the container for the testing data(specifying their labels as NULL/zero and how many of them are to be predicted/tested?)
# 7.  Providing the "testing" container of step 6 as input to the model created in step 4 to get output.
# 2.  Creating the DTM for training data.
# Mentioning the starting document index for training
start_doc_index_for_training = 1
# Mentioning the ending document index for training
end_doc_index_for_training = 100
trainSize = end_doc_index_for_training - start_doc_index_for_training + 1
train_db_corpus_for_SVM = c(corpus.ng.train.db[[1]]["content"][[1]])
if(length(db_train_docs)>1)
{
for(i in 2:length(db_train_docs))
{
train_db_corpus_for_SVM = c(train_db_corpus_for_SVM, paste(corpus.ng.train.db[[i]]["content"][[1]], collapse = ''))
}
}
dtMatrix.db <- create_matrix(train_db_corpus_for_SVM[start_doc_index_for_training:end_doc_index_for_training])
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
db_train_corpus_data_labels = rep(1,length(db_train_docs))
db_train_corpus_data_labels[c(1,11,17,36,52,69,79,80)] = 2
container.db <- create_container(dtMatrix.db, db_train_corpus_data_labels, trainSize=start_doc_index_for_training:end_doc_index_for_training, virgin=FALSE)
# 4.  Creating the model (SVM) using the container of step 3.
model.db <- train_model(container.db, "SVM", kernel="linear", cost=1)
types_of_labels = c(1, 2)
label_names = c("Accepted", "Rejected")
remove.packages("RTextTools", lib="~/R/x86_64-pc-linux-gnu-library/3.5")
detach("package:RTextTools", unload=TRUE)
library("RTextTools", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
#Obtaining the full path to the "Case Data Files" local directory
full_path_to_data_files = paste("/media/sidshenoy/KINGSTON/NIC work/SVM Classifier App", "/Case Data Files", sep = "")
#obtaining the names of all files ending with "pdf"
db_files <- list.files(path = full_path_to_data_files, pattern = "pdf$")
#obtaining the full paths to all the above files
db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument, lazy = TRUE)
# I am using SVM after cleaning the data (i.e. removal of stop words, punctuations, stemming, etc.) and on unigrams only!
# There is a problem with "RTextTools" package for ngrams, so I am using unigrams only.
# There are 7 steps associated with "RTextTools package" for classification ->
# 1.  Obtaining the data for both training and testing.
# 2.  Creating the DTM for training data.
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
# 4.  Creating the model using the container of step 3.
# 5.  Creating the DTM for testing data based on training data.
# 6.  Creating the container for the testing data(specifying their labels as NULL/zero and how many of them are to be predicted/tested?)
# 7.  Providing the "testing" container of step 6 as input to the model created in step 4 to get output.
# 2.  Creating the DTM for training data.
# Mentioning the starting document index for training
start_doc_index_for_training = 1
# Mentioning the ending document index for training
end_doc_index_for_training = 100
trainSize = end_doc_index_for_training - start_doc_index_for_training + 1
train_db_corpus_for_SVM = c(corpus.ng.train.db[[1]]["content"][[1]])
if(length(db_train_docs)>1)
{
for(i in 2:length(db_train_docs))
{
train_db_corpus_for_SVM = c(train_db_corpus_for_SVM, paste(corpus.ng.train.db[[i]]["content"][[1]], collapse = ''))
}
}
dtMatrix.db <- create_matrix(train_db_corpus_for_SVM[start_doc_index_for_training:end_doc_index_for_training])
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
db_train_corpus_data_labels = rep(1,length(db_train_docs))
db_train_corpus_data_labels[c(1,11,17,36,52,69,79,80)] = 2
container.db <- create_container(dtMatrix.db, db_train_corpus_data_labels, trainSize=start_doc_index_for_training:end_doc_index_for_training, virgin=FALSE)
# 4.  Creating the model (SVM) using the container of step 3.
model.db <- train_model(container.db, "SVM", kernel="linear", cost=1)
types_of_labels = c(1, 2)
label_names = c("Accepted", "Rejected")
detach("package:RTextTools", unload=TRUE)
remove.packages("RTextTools", lib="~/R/x86_64-pc-linux-gnu-library/3.5")
#Obtaining the full path to the "Case Data Files" local directory
full_path_to_data_files = paste("/media/sidshenoy/KINGSTON/NIC work/SVM Classifier App", "/Case Data Files", sep = "")
#obtaining the names of all files ending with "pdf"
db_files <- list.files(path = full_path_to_data_files, pattern = "pdf$")
#obtaining the full paths to all the above files
db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument, lazy = TRUE)
# I am using SVM after cleaning the data (i.e. removal of stop words, punctuations, stemming, etc.) and on unigrams only!
# There is a problem with "RTextTools" package for ngrams, so I am using unigrams only.
# There are 7 steps associated with "RTextTools package" for classification ->
# 1.  Obtaining the data for both training and testing.
# 2.  Creating the DTM for training data.
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
# 4.  Creating the model using the container of step 3.
# 5.  Creating the DTM for testing data based on training data.
# 6.  Creating the container for the testing data(specifying their labels as NULL/zero and how many of them are to be predicted/tested?)
# 7.  Providing the "testing" container of step 6 as input to the model created in step 4 to get output.
# 2.  Creating the DTM for training data.
# Mentioning the starting document index for training
start_doc_index_for_training = 1
# Mentioning the ending document index for training
end_doc_index_for_training = 100
trainSize = end_doc_index_for_training - start_doc_index_for_training + 1
train_db_corpus_for_SVM = c(corpus.ng.train.db[[1]]["content"][[1]])
if(length(db_train_docs)>1)
{
for(i in 2:length(db_train_docs))
{
train_db_corpus_for_SVM = c(train_db_corpus_for_SVM, paste(corpus.ng.train.db[[i]]["content"][[1]], collapse = ''))
}
}
dtMatrix.db <- create_matrix(train_db_corpus_for_SVM[start_doc_index_for_training:end_doc_index_for_training])
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
db_train_corpus_data_labels = rep(1,length(db_train_docs))
db_train_corpus_data_labels[c(1,11,17,36,52,69,79,80)] = 2
container.db <- create_container(dtMatrix.db, db_train_corpus_data_labels, trainSize=start_doc_index_for_training:end_doc_index_for_training, virgin=FALSE)
# 4.  Creating the model (SVM) using the container of step 3.
model.db <- train_model(container.db, "SVM", kernel="linear", cost=1)
types_of_labels = c(1, 2)
label_names = c("Accepted", "Rejected")
detach("package:RTextTools", unload=TRUE)
remove.packages("RTextTools", lib="~/R/x86_64-pc-linux-gnu-library/3.5")
setwd("../../RTextTools")
#Obtaining the full path to the "Case Data Files" local directory
full_path_to_data_files = paste("/media/sidshenoy/KINGSTON/NIC work/SVM Classifier App", "/Case Data Files", sep = "")
#obtaining the names of all files ending with "pdf"
db_files <- list.files(path = full_path_to_data_files, pattern = "pdf$")
#obtaining the full paths to all the above files
db_files_full_path <- paste(full_path_to_data_files,"/",db_files,sep="")
#creates a list object with three elements, one for each document. Each element is a vector that contains the text
#of the PDF file. The length of each vector corresponds to the number of pages in the PDF file.
db_train_docs <- lapply(db_files_full_path, pdf_text)
#If this step is skipped and you move directly to the next step, you may notice certain words are preceded with double quotes and dashes even though we specified removePunctuation = TRUE.
#We even see a series of dashes being treated as a word. What happened? It's hard to say exactly, but apparently the pdf_text function preserved the unicode curly-quotes and em-dashes used in
#the PDF files. Therefore the unicode characters were not removed because it appears punctuation removal in the tm package does not include certain unicode characters. With some investigation
#we determined the unicode for the opening and closing double quotes, the em-dash, the en-dash, opening and closing single quotes and horizontal ellipsis are \u201c, \u201d, \u2014, \u2018 ,
#\u2019 and \u2026. So we replace such characters with ""(NULL).
db_train_docs <- lapply(db_train_docs, function(x)gsub("(\u201c|\u201d|\u2013|\u2014|\u2018|\u2019|\u2026)","",x))
#Creating the VCorpus of all docs using the function created above
corp.train.db <- Corpus(VectorSource(db_train_docs))
#We need to manipulate/clean the corpus first so that it can be tokenized into words
corpus.ng.train.db <- tm_map(corp.train.db, content_transformer(tolower))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removePunctuation)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeNumbers)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stripWhitespace)
corpus.ng.train.db <- tm_map(corpus.ng.train.db, removeWords, stopwords('english'))
corpus.ng.train.db <- tm_map(corpus.ng.train.db, stemDocument, lazy = TRUE)
# I am using SVM after cleaning the data (i.e. removal of stop words, punctuations, stemming, etc.) and on unigrams only!
# There is a problem with "RTextTools" package for ngrams, so I am using unigrams only.
# There are 7 steps associated with "RTextTools package" for classification ->
# 1.  Obtaining the data for both training and testing.
# 2.  Creating the DTM for training data.
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
# 4.  Creating the model using the container of step 3.
# 5.  Creating the DTM for testing data based on training data.
# 6.  Creating the container for the testing data(specifying their labels as NULL/zero and how many of them are to be predicted/tested?)
# 7.  Providing the "testing" container of step 6 as input to the model created in step 4 to get output.
# 2.  Creating the DTM for training data.
# Mentioning the starting document index for training
start_doc_index_for_training = 1
# Mentioning the ending document index for training
end_doc_index_for_training = 100
trainSize = end_doc_index_for_training - start_doc_index_for_training + 1
train_db_corpus_for_SVM = c(corpus.ng.train.db[[1]]["content"][[1]])
if(length(db_train_docs)>1)
{
for(i in 2:length(db_train_docs))
{
train_db_corpus_for_SVM = c(train_db_corpus_for_SVM, paste(corpus.ng.train.db[[i]]["content"][[1]], collapse = ''))
}
}
dtMatrix.db <- create_matrix(train_db_corpus_for_SVM[start_doc_index_for_training:end_doc_index_for_training])
# 3.  Creating the container for the training data(specifying their labels and how many of them to consider for training)
db_train_corpus_data_labels = rep(1,length(db_train_docs))
db_train_corpus_data_labels[c(1,11,17,36,52,69,79,80)] = 2
container.db <- create_container(dtMatrix.db, db_train_corpus_data_labels, trainSize=start_doc_index_for_training:end_doc_index_for_training, virgin=FALSE)
# 4.  Creating the model (SVM) using the container of step 3.
model.db <- train_model(container.db, "SVM", kernel="linear", cost=1)
types_of_labels = c(1, 2)
label_names = c("Accepted", "Rejected")
